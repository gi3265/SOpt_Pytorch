{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "geographic-class",
   "metadata": {},
   "source": [
    "학습목표\n",
    "\n",
    "경사하강법(Gradient descent)에 대해 더 자세히 알아본다.\n",
    "핵심키워드\n",
    "\n",
    "* 가설 함수(Hypothesis Function)\n",
    "* 평균 제곱 오차(Mean Squared Error)\n",
    "* 경사하강법(Gradient descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "formed-morocco",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "american-defense",
   "metadata": {},
   "source": [
    "## Simplier Hypothesis Function\n",
    "H(x) = Wx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "deluxe-orbit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 파라미터 초기화\n",
    "W = torch.zeros(1, requires_grad = True)\n",
    "# b = torch.zeros(1, requires_grad = True)\n",
    "\n",
    "# 데이터셋 정의\n",
    "x_train = torch.FloatTensor([[1,2,3]])\n",
    "y_train = torch.FloatTensor([[1,2,3]])\n",
    "\n",
    "# 예측 모델 정의\n",
    "hypothesis = x_train * W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dimensional-syntax",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = torch.mean((hypothesis - y_train) ** 2)  # W = 1에서 최저값을 가지는 cost에 대한 2차 곡선"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "textile-salvation",
   "metadata": {},
   "source": [
    "## Gradient Descent: Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "intimate-welding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost함수를 W(변수)에 대해 미분한 gradient 함수 정의\n",
    "gradient = 2 * torch.sum((hypothesis - y_train) * x_train)\n",
    "lr = 0.1\n",
    "# Gradient 갱신\n",
    "W = -lr * gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imported-reproduction",
   "metadata": {},
   "source": [
    "## Gradient Descent: Full Code(w/o optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "departmental-virgin",
   "metadata": {},
   "source": [
    "gradient = 2 * torch.sum((hypothesis - y_train)* x_train) 에서 torch.mean보다 torch.sum으로 할때 수렴이 훨씬 빠르게 된다.\\\n",
    "왜? 손실 기울기는 '평균값'인 mse라는 loss값을 미분하여 얻어진 값인 만큼 수식에 m으로 나누는 부분이 포함된다. m으로 나누는 것이 W의 파라미터를 loss값이 줄어드는 방향으로 갱신하는데 직접적으로 어떤 역할을 가지는 것은 없다. 오히려 갱신되는 폭을 감소시키는 것이기에 변화 속도를 늦출 뿐이다. 그래서 아래 코드에서는 torch.sum함수를 사용했다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "romance-tackle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    1/10 W: 0.000, Cost: 4.666667\n",
      "Epoch:    2/10 W: 0.280, Cost: 2.419200\n",
      "Epoch:    3/10 W: 0.482, Cost: 1.254113\n",
      "Epoch:    4/10 W: 0.627, Cost: 0.650132\n",
      "Epoch:    5/10 W: 0.731, Cost: 0.337029\n",
      "Epoch:    6/10 W: 0.807, Cost: 0.174716\n",
      "Epoch:    7/10 W: 0.861, Cost: 0.090573\n",
      "Epoch:    8/10 W: 0.900, Cost: 0.046953\n",
      "Epoch:    9/10 W: 0.928, Cost: 0.024340\n",
      "Epoch:   10/10 W: 0.948, Cost: 0.012618\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "\n",
    "# 데이터셋 정의\n",
    "x_train = torch.FloatTensor([[1,2,3]])\n",
    "y_train = torch.FloatTensor([[1,2,3]])\n",
    "\n",
    "\n",
    "# 모델 파라미터 초기화\n",
    "W = torch.zeros(1) #requires_grad = True:[RuntimeError]a leaf Variable that requires grad is being used in an in-place operation.\n",
    "\n",
    "# learning rate 설정\n",
    "lr = 0.01\n",
    "\n",
    "# for loops 돌아가며 파라미터 갱신\n",
    "nb_epochs = 10\n",
    "for epoch in range(1, nb_epochs+1):\n",
    "    # 예측(H(x) 계산)\n",
    "    hypothesis = x_train * W\n",
    "    # loss와 loss gradient 계산\n",
    "    loss = torch.mean((hypothesis - y_train)**2) # 텐서들이므로 x = [1,2,3], y = [1,2,3] 통째로 연산\n",
    "    gradient = 2 * torch.sum((hypothesis - y_train)* x_train)\n",
    "    \n",
    "    print('Epoch: {:4d}/{} W: {:.3f}, Cost: {:.6f}'.format(epoch, nb_epochs, W.item(), loss.item()))\n",
    "    # loss gradient로 W 파라미터 갱신해주어 loss값 줄이기\n",
    "    W -= lr * gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grave-envelope",
   "metadata": {},
   "source": [
    "Epoch: 데이터로 학습한 횟수\\\n",
    "학습하면서 점점:\n",
    "   * 1에 수렴하는 W\n",
    "   * 줄어드는 cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "refined-evanescence",
   "metadata": {},
   "source": [
    "## Gradient Descent w/ torch.optim \n",
    "torch.optim 사용하면 loss gradient 직접적으로 계산해줄 필요 없음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "further-spyware",
   "metadata": {},
   "source": [
    "torch.optim 을 사용함으로서 gradient descent를 더 편하게 할 수 있다!\n",
    "  * 시작할 때 optimizer 정의\n",
    "  * optimizer.zero_grad()로 gradient를 0으로 초기화\n",
    "  * cost.bacward()로 loss gradient 계산\n",
    "  * optimizer.step()으로 gradient descent(실질적으로 W, b 파라미터 갱신하고 로스값 낮춤)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "filled-format",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    1/10 W: 0.000, Cost: 4.666667\n",
      "Epoch:    2/10 W: 1.400, Cost: 0.746667\n",
      "Epoch:    3/10 W: 0.840, Cost: 0.119467\n",
      "Epoch:    4/10 W: 1.064, Cost: 0.019115\n",
      "Epoch:    5/10 W: 0.974, Cost: 0.003058\n",
      "Epoch:    6/10 W: 1.010, Cost: 0.000489\n",
      "Epoch:    7/10 W: 0.996, Cost: 0.000078\n",
      "Epoch:    8/10 W: 1.002, Cost: 0.000013\n",
      "Epoch:    9/10 W: 0.999, Cost: 0.000002\n",
      "Epoch:   10/10 W: 1.000, Cost: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SGI\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:132: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  ..\\c10\\cuda\\CUDAFunctions.cpp:100.)\n",
      "  allow_unreachable=True)  # allow_unreachable flag\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "\n",
    "\n",
    "# 데이터셋 정의\n",
    "x_train = torch.FloatTensor([[1,2,3]])\n",
    "y_train = torch.FloatTensor([[1,2,3]])\n",
    "\n",
    "# 모델 파라미터 초기화\n",
    "W = torch.zeros(1, requires_grad = True)\n",
    "# b = torch.zeros(1, requires_grad = True)\n",
    "\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD([W], lr = 0.15)\n",
    "\n",
    "# for loops 돌아가며 파라미터 갱신\n",
    "nb_epochs = 10\n",
    "for epoch in range(1, nb_epochs+1):\n",
    "    # 예측 및 loss값 계산\n",
    "    hypothesis = W * x_train\n",
    "    loss = torch.mean((hypothesis - y_train)**2)\n",
    "    \n",
    "    print('Epoch: {:4d}/{} W: {:.3f}, Cost: {:.6f}'.format(epoch, nb_epochs, W.item(), loss.item()))\n",
    "    \n",
    "    # backward()매서드에 loss값 넣어 loss gradient값 계산하고 이 값을 optimizer.step()매서드에 넣어\n",
    "    # 결과적으로 loss 감소시키기(모델 학습)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dutch-shannon",
   "metadata": {},
   "source": [
    "Epoch: 데이터로 학습한 횟수\\\n",
    "학습하면서 점점:\n",
    "   * 1에 수렴하는 W\n",
    "   * 줄어드는 cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "touched-retention",
   "metadata": {},
   "source": [
    "지금까지 한 종류의 정보만을 이용해 예측하는 모델을 만들었다.\\\n",
    "다음 수업 부터는? 다양한 정보를 추합해서 예측하는 모델 만들어 볼 것"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
